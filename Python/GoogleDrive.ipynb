{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciona Lista Para Automatizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargado 100%.\n",
      "Archivo actualizado en Google Drive: InvoicePurchases12312016_Clean.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#___________________________________________________________________________________________________________________________________________________________\n",
    "\n",
    "#                                                       LIIBRERIAS NECESARIAS\n",
    "\n",
    "import os\n",
    "import csv\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "from googleapiclient.http import MediaFileUpload, MediaIoBaseDownload\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import urllib.parse  # Agregado para construir la cadena de conexión\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "\n",
    "#___________________________________________________________________________________________________________________________________________________________\n",
    "\n",
    "#                                                 RELLENO DE NULOS CARGA Y ACTUALIZACION ARCHIVOS GOOGLE DRIVE\n",
    "\n",
    "# Directorio donde se encuentra el archivo JSON de credenciales\n",
    "directorio_credenciales = \"data-424019-28bfddebf741.json\"\n",
    "\n",
    "# Cargar credenciales desde el archivo JSON descargado\n",
    "credentials = service_account.Credentials.from_service_account_file(directorio_credenciales)\n",
    "\n",
    "# Crear cliente para acceder a la API de Google Drive\n",
    "drive_service = build('drive', 'v3', credentials=credentials)\n",
    "\n",
    "# ID de la carpeta en Google Drive\n",
    "folder_id = '1tZHS0BuJoSG3DsfSmxgY5Am3Llj24JPY'\n",
    "\n",
    "# Función para descargar un archivo de Google Drive\n",
    "def descargar_archivo(file_id, download_path):\n",
    "    request = drive_service.files().get_media(fileId=file_id)\n",
    "    with open(download_path, 'wb') as fh:\n",
    "        downloader = MediaIoBaseDownload(fh, request)\n",
    "        done = False\n",
    "        while not done:\n",
    "            status, done = downloader.next_chunk()\n",
    "            print(f\"Descargado {int(status.progress() * 100)}%.\")\n",
    "\n",
    "# Función para subir un archivo a Google Drive\n",
    "def subir_archivo(file_id, upload_file_path):\n",
    "    media = MediaFileUpload(upload_file_path, resumable=True)\n",
    "    updated_file = drive_service.files().update(fileId=file_id, media_body=media).execute()\n",
    "    print('Archivo actualizado en Google Drive:', updated_file.get('name'))\n",
    "\n",
    "# Directorio temporal para manipular archivos\n",
    "temp_dir = tempfile.gettempdir()\n",
    "\n",
    "# Lista para almacenar los archivos modificados\n",
    "archivos_modificados = []\n",
    "\n",
    "# Obtener lista de archivos en la carpeta de Google Drive\n",
    "results = drive_service.files().list(q=f\"'{folder_id}' in parents\", fields='files(id, name)').execute()\n",
    "files = results.get('files', [])\n",
    "\n",
    "# Iterar sobre los archivos\n",
    "for file in files:\n",
    "    file_id = file['id']\n",
    "    file_name = file['name']\n",
    "    download_path = os.path.join(temp_dir, file_name)\n",
    "\n",
    "    # Descargar el archivo de Google Drive\n",
    "    descargar_archivo(file_id, download_path)\n",
    "\n",
    "    # Leer el archivo CSV y hacer modificaciones\n",
    "    with open(download_path, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        rows = list(reader)\n",
    "        for row_index, row in enumerate(rows):\n",
    "            for col_index, cell in enumerate(row):\n",
    "                if not cell:\n",
    "                    rows[row_index][col_index] = \"no definido\"\n",
    "\n",
    "    # Guardar los cambios en el archivo temporal sin la extensión _temp.csv\n",
    "    download_path_temp = os.path.join(temp_dir, os.path.splitext(file_name)[0])\n",
    "    with open(download_path_temp, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerows(rows)\n",
    "\n",
    "    archivos_modificados.append((file_id, download_path_temp))\n",
    "\n",
    "# Subir los archivos modificados a Google Drive\n",
    "for file_id, download_path_temp in archivos_modificados:\n",
    "    subir_archivo(file_id, download_path_temp)\n",
    "\n",
    "# Guardar la marca de tiempo de la última ejecución\n",
    "# Puedes cambiar esto según cómo desees formatear la marca de tiempo\n",
    "marca_de_tiempo_actual = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "with open(\"ultima_ejecucion.txt\", \"w\") as file:\n",
    "    file.write(marca_de_tiempo_actual)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargando archivo 'Carpeta de origen/InvoicePurchases12312016_Clean.csv' desde la carpeta 'Carpeta de origen'.\n",
      "Datos del archivo de origen 'InvoicePurchases12312016_Clean.csv':\n",
      "Descargando archivo 'Carpeta de origen/2017PurchasePricesDec_Clean.csv' desde la carpeta 'Carpeta de origen'.\n",
      "Datos del archivo de origen '2017PurchasePricesDec_Clean.csv':\n",
      "Descargando archivo 'Carpeta de comparación/comparison_2017PurchasePricesDec_Clean.csv' desde la carpeta 'Carpeta de comparación'.\n",
      "Datos del archivo de comparación '2017PurchasePricesDec_Clean.csv':\n",
      "Descargando archivo 'Carpeta de comparación/comparison_InvoicePurchases12312016_Clean.csv' desde la carpeta 'Carpeta de comparación'.\n",
      "Datos del archivo de comparación 'InvoicePurchases12312016_Clean.csv':\n",
      "Realizando comparación entre los datos de '2017PurchasePricesDec_Clean.csv' en la carpeta de origen y en la carpeta de comparación.\n",
      "Se encontraron filas nuevas en el archivo '2017PurchasePricesDec_Clean.csv':\n",
      "Archivo '2017PurchasePricesDec_Clean.csv' eliminado de la carpeta de destino.\n",
      "Archivo 2017PurchasePricesDec_Clean.csv subido a Google Drive en la carpeta especificada: 18gXDMFkbWoGDXfRN6JVcmQdi791cHZ9G\n",
      "El archivo '2017PurchasePricesDec_Clean.csv' con las filas nuevas se ha subido correctamente a la carpeta de destino.\n",
      "Realizando comparación entre los datos de 'InvoicePurchases12312016_Clean.csv' en la carpeta de origen y en la carpeta de comparación.\n",
      "No se encontraron filas nuevas en el archivo 'InvoicePurchases12312016_Clean.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Directorio donde se encuentra el archivo JSON de credenciales\n",
    "directorio_credenciales = \"data-424019-28bfddebf741.json\"\n",
    "\n",
    "# Cargar credenciales desde el archivo JSON descargado\n",
    "credentials = service_account.Credentials.from_service_account_file(directorio_credenciales)\n",
    "\n",
    "# Crear cliente para acceder a la API de Google Drive\n",
    "drive_service = build('drive', 'v3', credentials=credentials)\n",
    "\n",
    "# IDs de las carpetas en Google Drive\n",
    "source_folder_id = '1tZHS0BuJoSG3DsfSmxgY5Am3Llj24JPY'  # ID de la carpeta de destino\n",
    "comparison_folder_id = '1ZMAqOISfz-z5kv_4OgGZT5lI1C9S3Xwb'  # ID de la carpeta de comparación\n",
    "destination_folder_id = '18gXDMFkbWoGDXfRN6JVcmQdi791cHZ9G'  # ID de la carpeta de origen\n",
    "\n",
    "\n",
    "\n",
    "# Función para descargar un archivo de Google Drive\n",
    "def descargar_archivo(file_id, download_path, folder_name):\n",
    "    request = drive_service.files().get_media(fileId=file_id)\n",
    "    with open(download_path, 'wb') as fh:\n",
    "        downloader = MediaIoBaseDownload(fh, request)\n",
    "        done = False\n",
    "        while not done:\n",
    "            status, done = downloader.next_chunk()\n",
    "            print(f\"Descargando archivo '{folder_name}/{os.path.basename(download_path)}' desde la carpeta '{folder_name}'.\")\n",
    "\n",
    "# Función para subir un archivo a una carpeta específica en Google Drive\n",
    "def subir_archivo_a_carpeta(upload_file_path, folder_id, file_name):\n",
    "    media = MediaFileUpload(upload_file_path, resumable=True)\n",
    "    file_metadata = {\n",
    "        'name': file_name,\n",
    "        'parents': [folder_id]\n",
    "    }\n",
    "    drive_service.files().create(body=file_metadata, media_body=media, fields='id').execute()\n",
    "    print(f'Archivo {file_name} subido a Google Drive en la carpeta especificada: {folder_id}')\n",
    "\n",
    "# Directorio temporal para manipular archivos\n",
    "temp_dir = tempfile.gettempdir()\n",
    "\n",
    "# Obtener lista de archivos en la carpeta de origen de Google Drive\n",
    "results = drive_service.files().list(q=f\"'{source_folder_id}' in parents\", fields='files(id, name)').execute()\n",
    "source_files = results.get('files', [])\n",
    "\n",
    "# Leer los archivos de la carpeta de origen y almacenarlos en un diccionario\n",
    "origen_data = {}\n",
    "for file in source_files:\n",
    "    file_id = file['id']\n",
    "    file_name = file['name']\n",
    "    download_path_origen = os.path.join(temp_dir, file_name)\n",
    "    \n",
    "    # Descargar archivo de la carpeta de origen\n",
    "    descargar_archivo(file_id, download_path_origen, 'Carpeta de origen')\n",
    "    \n",
    "    # Leer el archivo CSV y almacenar los datos en el diccionario\n",
    "    origen_data[file_name] = pd.read_csv(download_path_origen)\n",
    "    print(f\"Datos del archivo de origen '{file_name}':\")\n",
    "\n",
    "# Obtener lista de archivos en la carpeta de comparación de Google Drive\n",
    "results_comparison = drive_service.files().list(q=f\"'{comparison_folder_id}' in parents\", fields='files(id, name)').execute()\n",
    "comparison_files_list = results_comparison.get('files', [])\n",
    "\n",
    "# Definir comparison_files como un diccionario vacío para almacenar los nombres de archivo y sus datos\n",
    "comparison_files = {}\n",
    "\n",
    "# Leer los archivos de la carpeta de comparación y almacenarlos en un diccionario\n",
    "for file in comparison_files_list:\n",
    "    file_id = file['id']\n",
    "    file_name = file['name']\n",
    "    download_path_comparacion = os.path.join(temp_dir, f\"comparison_{file_name}\")\n",
    "    \n",
    "    # Descargar archivo de la carpeta de comparación\n",
    "    descargar_archivo(file_id, download_path_comparacion, 'Carpeta de comparación')\n",
    "    \n",
    "    # Leer el archivo CSV y almacenar los datos en el diccionario\n",
    "    comparison_files[file_name] = pd.read_csv(download_path_comparacion)\n",
    "    print(f\"Datos del archivo de comparación '{file_name}':\")\n",
    "    \n",
    "# Realizar la comparación entre los datos de origen y los datos de comparación\n",
    "for file_name, df_comparacion in comparison_files.items():\n",
    "    if file_name in origen_data:\n",
    "        print(f\"Realizando comparación entre los datos de '{file_name}' en la carpeta de origen y en la carpeta de comparación.\")\n",
    "        \n",
    "        df_origen = origen_data[file_name]\n",
    "        \n",
    "        # Verificar y convertir las columnas para que tengan el mismo tipo de datos\n",
    "        columnas_comunes = list(set(df_origen.columns) & set(df_comparacion.columns))\n",
    "        for columna in columnas_comunes:\n",
    "            tipo_origen = df_origen[columna].dtype\n",
    "            tipo_comparacion = df_comparacion[columna].dtype\n",
    "            if tipo_origen != tipo_comparacion:\n",
    "                if tipo_origen == 'object' or tipo_comparacion == 'object':\n",
    "                    # Convertir a tipo de datos string si alguna de las columnas es de tipo object\n",
    "                    df_origen[columna] = df_origen[columna].astype(str)\n",
    "                    df_comparacion[columna] = df_comparacion[columna].astype(str)\n",
    "                else:\n",
    "                    # Convertir ambas columnas al tipo de datos más general (por ejemplo, float64)\n",
    "                    tipo_general = max(tipo_origen, tipo_comparacion)\n",
    "                    df_origen[columna] = df_origen[columna].astype(tipo_general)\n",
    "                    df_comparacion[columna] = df_comparacion[columna].astype(tipo_general)\n",
    "\n",
    "        # Comparar y obtener las filas nuevas (las filas que están en df_origen pero no en df_comparacion)\n",
    "        df_nuevas_filas = df_origen.merge(df_comparacion, indicator=True, how='left').loc[lambda x: x['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "        if not df_nuevas_filas.empty:\n",
    "            print(f\"Se encontraron filas nuevas en el archivo '{file_name}':\")\n",
    "            \n",
    "\n",
    "                # Función para subir un archivo a una carpeta específica en Google Drive\n",
    "            def subir_archivo_a_carpeta(upload_file_path, folder_id, file_name):\n",
    "                # Borrar todos los archivos existentes en la carpeta de destino\n",
    "                files = drive_service.files().list(q=f\"'{folder_id}' in parents\", fields='files(id)').execute()\n",
    "                for file in files.get('files', []):\n",
    "                    drive_service.files().delete(fileId=file.get('id')).execute()\n",
    "                    print(f\"Archivo '{file_name}' eliminado de la carpeta de destino.\")\n",
    "\n",
    "                # Subir el archivo especificado a la carpeta de destino\n",
    "                media = MediaFileUpload(upload_file_path, resumable=True)\n",
    "                file_metadata = {\n",
    "                    'name': file_name,\n",
    "                    'parents': [folder_id]\n",
    "                }\n",
    "                drive_service.files().create(body=file_metadata, media_body=media, fields='id').execute()\n",
    "                print(f'Archivo {file_name} subido a Google Drive en la carpeta especificada: {folder_id}')\n",
    "\n",
    "            # Subir el DataFrame con las filas nuevas a la carpeta de destino\n",
    "            nuevo_nombre_path = os.path.join(temp_dir, file_name)\n",
    "            with open(nuevo_nombre_path, 'w', newline='') as file:\n",
    "                df_nuevas_filas.to_csv(file, index=False)\n",
    "            subir_archivo_a_carpeta(nuevo_nombre_path, destination_folder_id, file_name)\n",
    "            print(f\"El archivo '{file_name}' con las filas nuevas se ha subido correctamente a la carpeta de destino.\")\n",
    "        else:\n",
    "            print(f\"No se encontraron filas nuevas en el archivo '{file_name}'.\")\n",
    "    else:\n",
    "        print(f\"No se encontró el archivo '{file_name}' en la carpeta de origen.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed old CSV file: InvoicePurchases12312016_Clean.csv\n",
      "Removed old CSV file: new_InvoicePurchases12312016_Clean.csv\n",
      "Downloaded 100% of InvoicePurchases12312016_Clean.csv\n",
      "Se han cambiado los delimitadores de todos los archivos CSV en el directorio por ';'.\n",
      "Data from InvoicePurchases12312016_Clean.csv loaded into SQL table InvoicePurchases12312016_Clean\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#___________________________________________________________________________________________________________________________________________________________\n",
    "\n",
    "#                                               CARGA DE DATOS SQL\n",
    "\n",
    "# Definir la ruta al archivo JSON de credenciales de Google Drive\n",
    "credentials_file = 'data-424019-28bfddebf741.json'\n",
    "\n",
    "# Cargar las credenciales desde el archivo JSON\n",
    "credentials = service_account.Credentials.from_service_account_file(credentials_file)\n",
    "\n",
    "# Crear un cliente para acceder a la API de Google Drive\n",
    "drive_service = build('drive', 'v3', credentials=credentials)\n",
    "\n",
    "# ID de la carpeta en Google Drive donde se encuentran los archivos CSV\n",
    "folder_id = '18gXDMFkbWoGDXfRN6JVcmQdi791cHZ9G'\n",
    "\n",
    "# Directorio base donde se guardarán los archivos CSV descargados\n",
    "base_directory = '/ruta/a/tu/directorio/base'\n",
    "csv_folder_name = 'archivos_csv_google_drive'\n",
    "local_directory = os.path.join(base_directory, csv_folder_name)\n",
    "\n",
    "# Función para descargar archivos CSV de Google Drive\n",
    "def download_csv_files_from_drive(folder_id, local_directory):\n",
    "    # Eliminar archivos CSV antiguos en el directorio local\n",
    "    for file in os.listdir(local_directory):\n",
    "        if file.endswith('.csv'):\n",
    "            os.remove(os.path.join(local_directory, file))\n",
    "            print(f\"Removed old CSV file: {file}\")\n",
    "    \n",
    "    # Descargar los nuevos archivos CSV desde Google Drive\n",
    "    results = drive_service.files().list(q=f\"'{folder_id}' in parents\", fields='files(id, name)').execute()\n",
    "    files = results.get('files', [])\n",
    "    for file in files:\n",
    "        file_id = file['id']\n",
    "        file_name = file['name']\n",
    "        download_path = os.path.join(local_directory, file_name)\n",
    "        request = drive_service.files().get_media(fileId=file_id)\n",
    "        with open(download_path, 'wb') as fh:\n",
    "            downloader = MediaIoBaseDownload(fh, request)\n",
    "            done = False\n",
    "            while not done:\n",
    "                status, done = downloader.next_chunk()\n",
    "                print(f\"Downloaded {int(status.progress() * 100)}% of {file_name}\")\n",
    "\n",
    "    # Cambiar el delimitador de todos los archivos CSV en el directorio\n",
    "    # Obtener la lista de archivos CSV en el directorio\n",
    "    csv_files = os.listdir(local_directory)\n",
    "\n",
    "    # Iterar sobre cada archivo CSV y cambiar el delimitador\n",
    "    for file_name in csv_files:\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(local_directory, file_name)\n",
    "            # Leer el archivo CSV con la coma como delimitador original\n",
    "            df = pd.read_csv(file_path)\n",
    "            # Cambiar el delimitador de la coma a punto y coma\n",
    "            df.to_csv(file_path, sep=';', index=False)\n",
    "\n",
    "    print(\"Se han cambiado los delimitadores de todos los archivos CSV en el directorio por ';'.\")\n",
    "\n",
    "\n",
    "# Función para cargar datos desde archivos CSV a una base de datos SQL Server\n",
    "def load_csv_data_to_sql(csv_directory, conn_str):\n",
    "    files = os.listdir(csv_directory)\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            file_path = os.path.join(csv_directory, file)\n",
    "            table_name = os.path.splitext(file)[0]  # Utilizar el nombre del archivo CSV como nombre de la tabla\n",
    "            \n",
    "            # Define el delimitador predeterminado\n",
    "            delimiter = ';'\n",
    "\n",
    "                \n",
    "            # Define una función de redondeo personalizada\n",
    "            def custom_round(value):\n",
    "                if isinstance(value, float):\n",
    "                    return round(value, 2)\n",
    "                return value\n",
    "            \n",
    "            # Leer el archivo CSV con el delimitador correcto y aplicar la función de redondeo personalizada\n",
    "            df = pd.read_csv(file_path, delimiter=delimiter, converters={i: custom_round for i in range(10)})  # ajusta el rango 10\n",
    "            \n",
    "            # Crear el motor de conexión a la base de datos\n",
    "            engine = create_engine(conn_str)\n",
    "            \n",
    "            # Cargar los datos en SQL Server\n",
    "            df.to_sql(name=table_name, con=engine, if_exists='append', index=False)\n",
    "            print(f\"Data from {file} loaded into SQL table {table_name}\")\n",
    "\n",
    "# Ejemplo de uso\n",
    "if __name__ == \"__main__\":\n",
    "    # Crear el directorio local si no existe\n",
    "    if not os.path.exists(local_directory):\n",
    "        os.makedirs(local_directory)\n",
    "\n",
    "    # Descargar archivos CSV de Google Drive a un directorio local\n",
    "    download_csv_files_from_drive(folder_id, local_directory)\n",
    "    \n",
    "    # Construir la cadena de conexión para SQLAlchemy\n",
    "    params = urllib.parse.quote_plus(\"DRIVER={ODBC Driver 17 for SQL Server};SERVER=JULIAN;DATABASE=Top-Drinks;Trusted_Connection=yes;\")\n",
    "    conn_str = f\"mssql+pyodbc:///?odbc_connect={params}\"\n",
    "    \n",
    "    # Cargar datos desde archivos CSV a una base de datos SQL Server\n",
    "    load_csv_data_to_sql(local_directory, conn_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargado 100%.\n",
      "Archivo actualizado en Google Drive: InvoicePurchases12312016_Clean.csv\n",
      "Archivos descargados y sobrescritos en Google Drive.\n"
     ]
    }
   ],
   "source": [
    "#___________________________________________________________________________________________________________________________________________________________\n",
    "\n",
    "#                                                       LIBRERIAS NECESARIAS\n",
    "\n",
    "import os\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaIoBaseDownload, MediaFileUpload\n",
    "import tempfile\n",
    "\n",
    "#___________________________________________________________________________________________________________________________________________________________\n",
    "\n",
    "#                                                 DESCARGA Y CARGA DE ARCHIVOS ENTRE CARPETAS DE GOOGLE DRIVE\n",
    "\n",
    "# Directorio donde se encuentra el archivo JSON de credenciales\n",
    "directorio_credenciales = \"data-424019-28bfddebf741.json\"\n",
    "\n",
    "# Cargar credenciales desde el archivo JSON descargado\n",
    "credentials = service_account.Credentials.from_service_account_file(directorio_credenciales)\n",
    "\n",
    "# Crear cliente para acceder a la API de Google Drive\n",
    "drive_service = build('drive', 'v3', credentials=credentials)\n",
    "\n",
    "# ID de la carpeta de origen en Google Drive\n",
    "source_folder_id = '1tZHS0BuJoSG3DsfSmxgY5Am3Llj24JPY'\n",
    "# ID de la carpeta de destino en Google Drive\n",
    "destination_id = '1ZMAqOISfz-z5kv_4OgGZT5lI1C9S3Xwb'  # Reemplaza 'your_destination_folder_id' con el ID de tu carpeta de destino\n",
    "\n",
    "# Directorio temporal para manipular archivos\n",
    "temp_dir = tempfile.gettempdir()\n",
    "\n",
    "# Función para descargar un archivo de Google Drive\n",
    "def descargar_archivo(file_id, download_path):\n",
    "    request = drive_service.files().get_media(fileId=file_id)\n",
    "    with open(download_path, 'wb') as fh:\n",
    "        downloader = MediaIoBaseDownload(fh, request)\n",
    "        done = False\n",
    "        while not done:\n",
    "            status, done = downloader.next_chunk()\n",
    "            print(f\"Descargado {int(status.progress() * 100)}%.\")\n",
    "\n",
    "# Función para subir un archivo a una carpeta específica en Google Drive\n",
    "def subir_archivo_a_carpeta(upload_file_path, file_name):\n",
    "    # Buscar el archivo en la carpeta de destino por su nombre\n",
    "    response = drive_service.files().list(\n",
    "        q=f\"name='{file_name}' and '{destination_id}' in parents\",\n",
    "        fields='files(id)'\n",
    "    ).execute()\n",
    "    \n",
    "    # Si se encuentra el archivo, obtener su ID y actualizarlo\n",
    "    if response.get('files', []):\n",
    "        file_id = response['files'][0]['id']\n",
    "        media = MediaFileUpload(upload_file_path)\n",
    "        drive_service.files().update(fileId=file_id, media_body=media).execute()\n",
    "        print(f'Archivo actualizado en Google Drive: {file_name}')\n",
    "    else:\n",
    "        # Si el archivo no existe, crear uno nuevo en la carpeta de destino\n",
    "        media = MediaFileUpload(upload_file_path, resumable=True)\n",
    "        file_metadata = {\n",
    "            'name': file_name,\n",
    "            'parents': [destination_id]\n",
    "        }\n",
    "        drive_service.files().create(body=file_metadata, media_body=media, fields='id').execute()\n",
    "        print(f'Archivo subido a Google Drive en la carpeta especificada: {destination_id}')\n",
    "\n",
    "# Obtener lista de archivos en la carpeta de origen de Google Drive\n",
    "results = drive_service.files().list(q=f\"'{source_folder_id}' in parents\", fields='files(id, name)').execute()\n",
    "files = results.get('files', [])\n",
    "\n",
    "# Iterar sobre los archivos, descargarlos y luego cargarlos en la carpeta de destino\n",
    "for file in files:\n",
    "    file_name = file['name']\n",
    "    download_path = os.path.join(temp_dir, file_name)\n",
    "\n",
    "    # Descargar el archivo de Google Drive\n",
    "    descargar_archivo(file['id'], download_path)\n",
    "\n",
    "    # Subir el archivo a la carpeta de destino\n",
    "    subir_archivo_a_carpeta(download_path, file_name)\n",
    "\n",
    "print(\"Archivos descargados y sobrescritos en Google Drive.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pruebas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### funciona pero sube todo el archivo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargando archivo 'Carpeta de origen/InvoicePurchases12312016_Clean.csv' desde la carpeta 'Carpeta de origen'.\n",
      "Datos del archivo de origen 'InvoicePurchases12312016_Clean.csv':\n",
      "   VendorNumber                   VendorName InvoiceDate PONumber      PODate  \\\n",
      "0           105  ALTAMAR BRANDS LLC           2016-01-04     8124  2015-12-21   \n",
      "1          4466  AMERICAN VINTAGE BEVERAGE    2016-01-07     8137  2015-12-22   \n",
      "2           388  ATLANTIC IMPORTING COMPANY   2016-01-09     8169  2015-12-24   \n",
      "3           480  BACARDI USA INC              2016-01-12     8106  2015-12-20   \n",
      "4           516  BANFI PRODUCTS CORP          2016-01-07     8170  2015-12-24   \n",
      "\n",
      "      PayDate Quantity    Dollars  Freight  \n",
      "0  2016-02-16        6     214.26     3.47  \n",
      "1  2016-02-21       15     140.55     8.57  \n",
      "2  2016-02-16        5      106.6     4.61  \n",
      "3  2016-02-05    10100  137483.78  2935.20  \n",
      "4  2016-02-12     1935   15527.25   429.20  \n",
      "Descargando archivo 'Carpeta de origen/PurchasesFINAL12312016_Clean.csv' desde la carpeta 'Carpeta de origen'.\n",
      "Descargando archivo 'Carpeta de origen/PurchasesFINAL12312016_Clean.csv' desde la carpeta 'Carpeta de origen'.\n",
      "Descargando archivo 'Carpeta de origen/PurchasesFINAL12312016_Clean.csv' desde la carpeta 'Carpeta de origen'.\n",
      "Descargando archivo 'Carpeta de origen/PurchasesFINAL12312016_Clean.csv' desde la carpeta 'Carpeta de origen'.\n",
      "Datos del archivo de origen 'PurchasesFINAL12312016_Clean.csv':\n",
      "           InventoryId  Store  Brand                   Description   Size  \\\n",
      "0    69_MOUNTMEND_8412     69   8412     Tequila Ocho Plata Fresno  750mL   \n",
      "1     30_CULCHETH_5255     30   5255  TGI Fridays Ultimte Mudslide  1.75L   \n",
      "2    34_PITMERDEN_5215     34   5215  TGI Fridays Long Island Iced  1.75L   \n",
      "3  1_HARDERSFIELD_5255      1   5255  TGI Fridays Ultimte Mudslide  1.75L   \n",
      "4    76_DONCASTER_2034     76   2034     Glendalough Double Barrel  750mL   \n",
      "\n",
      "   VendorNumber                   VendorName  PONumber      PODate  \\\n",
      "0           105  ALTAMAR BRANDS LLC               8124  2015-12-21   \n",
      "1          4466  AMERICAN VINTAGE BEVERAGE        8137  2015-12-22   \n",
      "2          4466  AMERICAN VINTAGE BEVERAGE        8137  2015-12-22   \n",
      "3          4466  AMERICAN VINTAGE BEVERAGE        8137  2015-12-22   \n",
      "4           388  ATLANTIC IMPORTING COMPANY       8169  2015-12-24   \n",
      "\n",
      "  ReceivingDate InvoiceDate     PayDate  PurchasePrice  Quantity  Dollars  \\\n",
      "0    2016-01-02  2016-01-04  2016-02-16          35.71         6   214.26   \n",
      "1    2016-01-01  2016-01-07  2016-02-21           9.35         4    37.40   \n",
      "2    2016-01-02  2016-01-07  2016-02-21           9.41         5    47.05   \n",
      "3    2016-01-01  2016-01-07  2016-02-21           9.35         6    56.10   \n",
      "4    2016-01-02  2016-01-09  2016-02-16          21.32         5   106.60   \n",
      "\n",
      "   Classification  \n",
      "0               1  \n",
      "1               1  \n",
      "2               1  \n",
      "3               1  \n",
      "4               1  \n",
      "Descargando archivo 'Carpeta de comparación/comparison_InvoicePurchases12312016_Clean.csv' desde la carpeta 'Carpeta de comparación'.\n",
      "Datos del archivo de comparación 'InvoicePurchases12312016_Clean.csv':\n",
      "   VendorNumber                   VendorName InvoiceDate  PONumber  \\\n",
      "0           105  ALTAMAR BRANDS LLC           2016-01-04      8124   \n",
      "1          4466  AMERICAN VINTAGE BEVERAGE    2016-01-07      8137   \n",
      "2           388  ATLANTIC IMPORTING COMPANY   2016-01-09      8169   \n",
      "3           480  BACARDI USA INC              2016-01-12      8106   \n",
      "4           516  BANFI PRODUCTS CORP          2016-01-07      8170   \n",
      "\n",
      "       PODate     PayDate  Quantity    Dollars  Freight  \n",
      "0  2015-12-21  2016-02-16         6     214.26     3.47  \n",
      "1  2015-12-22  2016-02-21        15     140.55     8.57  \n",
      "2  2015-12-24  2016-02-16         5     106.60     4.61  \n",
      "3  2015-12-20  2016-02-05     10100  137483.78  2935.20  \n",
      "4  2015-12-24  2016-02-12      1935   15527.25   429.20  \n",
      "Descargando archivo 'Carpeta de comparación/comparison_PurchasesFINAL12312016_Clean.csv' desde la carpeta 'Carpeta de comparación'.\n",
      "Descargando archivo 'Carpeta de comparación/comparison_PurchasesFINAL12312016_Clean.csv' desde la carpeta 'Carpeta de comparación'.\n",
      "Descargando archivo 'Carpeta de comparación/comparison_PurchasesFINAL12312016_Clean.csv' desde la carpeta 'Carpeta de comparación'.\n",
      "Descargando archivo 'Carpeta de comparación/comparison_PurchasesFINAL12312016_Clean.csv' desde la carpeta 'Carpeta de comparación'.\n",
      "Datos del archivo de comparación 'PurchasesFINAL12312016_Clean.csv':\n",
      "           InventoryId  Store  Brand                   Description   Size  \\\n",
      "0    69_MOUNTMEND_8412     69   8412     Tequila Ocho Plata Fresno  750mL   \n",
      "1     30_CULCHETH_5255     30   5255  TGI Fridays Ultimte Mudslide  1.75L   \n",
      "2    34_PITMERDEN_5215     34   5215  TGI Fridays Long Island Iced  1.75L   \n",
      "3  1_HARDERSFIELD_5255      1   5255  TGI Fridays Ultimte Mudslide  1.75L   \n",
      "4    76_DONCASTER_2034     76   2034     Glendalough Double Barrel  750mL   \n",
      "\n",
      "   VendorNumber                   VendorName  PONumber      PODate  \\\n",
      "0           105  ALTAMAR BRANDS LLC               8124  2015-12-21   \n",
      "1          4466  AMERICAN VINTAGE BEVERAGE        8137  2015-12-22   \n",
      "2          4466  AMERICAN VINTAGE BEVERAGE        8137  2015-12-22   \n",
      "3          4466  AMERICAN VINTAGE BEVERAGE        8137  2015-12-22   \n",
      "4           388  ATLANTIC IMPORTING COMPANY       8169  2015-12-24   \n",
      "\n",
      "  ReceivingDate InvoiceDate     PayDate  PurchasePrice  Quantity  Dollars  \\\n",
      "0    2016-01-02  2016-01-04  2016-02-16          35.71         6   214.26   \n",
      "1    2016-01-01  2016-01-07  2016-02-21           9.35         4    37.40   \n",
      "2    2016-01-02  2016-01-07  2016-02-21           9.41         5    47.05   \n",
      "3    2016-01-01  2016-01-07  2016-02-21           9.35         6    56.10   \n",
      "4    2016-01-02  2016-01-09  2016-02-16          21.32         5   106.60   \n",
      "\n",
      "   Classification  \n",
      "0               1  \n",
      "1               1  \n",
      "2               1  \n",
      "3               1  \n",
      "4               1  \n",
      "Descargando archivo 'Carpeta de comparación/comparison_SalesFINAL12312016_Clean.csv' desde la carpeta 'Carpeta de comparación'.\n",
      "Descargando archivo 'Carpeta de comparación/comparison_SalesFINAL12312016_Clean.csv' desde la carpeta 'Carpeta de comparación'.\n",
      "Datos del archivo de comparación 'SalesFINAL12312016_Clean.csv':\n",
      "           InventoryId  Store  Brand                 Description        Size  \\\n",
      "0  1_HARDERSFIELD_1004      1   1004  Jim Beam w/2 Rocks Glasses       750mL   \n",
      "1  1_HARDERSFIELD_1004      1   1004  Jim Beam w/2 Rocks Glasses       750mL   \n",
      "2  1_HARDERSFIELD_1004      1   1004  Jim Beam w/2 Rocks Glasses       750mL   \n",
      "3  1_HARDERSFIELD_1004      1   1004  Jim Beam w/2 Rocks Glasses       750mL   \n",
      "4  1_HARDERSFIELD_1005      1   1005     Maker's Mark Combo Pack  375mL 2 Pk   \n",
      "\n",
      "   SalesQuantity  SalesDollars  SalesPrice SalesDate  Volume  Classification  \\\n",
      "0              1         16.49       16.49  1/1/2016     750               1   \n",
      "1              2         32.98       16.49  1/2/2016     750               1   \n",
      "2              1         16.49       16.49  1/3/2016     750               1   \n",
      "3              1         14.49       14.49  1/8/2016     750               1   \n",
      "4              2         69.98       34.99  1/9/2016     375               1   \n",
      "\n",
      "   ExciseTax  VendorNo                   VendorName  \n",
      "0       0.79     12546  JIM BEAM BRANDS COMPANY      \n",
      "1       1.57     12546  JIM BEAM BRANDS COMPANY      \n",
      "2       0.79     12546  JIM BEAM BRANDS COMPANY      \n",
      "3       0.79     12546  JIM BEAM BRANDS COMPANY      \n",
      "4       0.79     12546  JIM BEAM BRANDS COMPANY      \n",
      "Descargando archivo 'Carpeta de comparación/comparison_BegInvFINAL12312016_Clean.csv' desde la carpeta 'Carpeta de comparación'.\n",
      "Datos del archivo de comparación 'BegInvFINAL12312016_Clean.csv':\n",
      "         InventoryId  Store          City  Brand                  Description  \\\n",
      "0  1_HARDERSFIELD_58      1  HARDERSFIELD     58  Gekkeikan Black & Gold Sake   \n",
      "1  1_HARDERSFIELD_60      1  HARDERSFIELD     60       Canadian Club 1858 VAP   \n",
      "2  1_HARDERSFIELD_62      1  HARDERSFIELD     62     Herradura Silver Tequila   \n",
      "3  1_HARDERSFIELD_63      1  HARDERSFIELD     63   Herradura Reposado Tequila   \n",
      "4  1_HARDERSFIELD_72      1  HARDERSFIELD     72         No. 3 London Dry Gin   \n",
      "\n",
      "    Size  onHand  Price   startDate  \n",
      "0  750mL       8  12.99  2016-01-01  \n",
      "1  750mL       7  10.99  2016-01-01  \n",
      "2  750mL       6  36.99  2016-01-01  \n",
      "3  750mL       3  38.99  2016-01-01  \n",
      "4  750mL       6  34.99  2016-01-01  \n",
      "Descargando archivo 'Carpeta de comparación/comparison_2017PurchasePricesDec_Clean.csv' desde la carpeta 'Carpeta de comparación'.\n",
      "Datos del archivo de comparación '2017PurchasePricesDec_Clean.csv':\n",
      "   Brand                  Description  Price   Size Volume  Classification  \\\n",
      "0     58  Gekkeikan Black & Gold Sake  12.99  750mL    750               1   \n",
      "1     62     Herradura Silver Tequila  36.99  750mL    750               1   \n",
      "2     63   Herradura Reposado Tequila  38.99  750mL    750               1   \n",
      "3     72         No. 3 London Dry Gin  34.99  750mL    750               1   \n",
      "4     75    Three Olives Tomato Vodka  14.99  750mL    750               1   \n",
      "\n",
      "   PurchasePrice  VendorNumber                   VendorName  \n",
      "0           9.28          8320  SHAW ROSS INT L IMP LTD      \n",
      "1          28.67          1128  BROWN-FORMAN CORP            \n",
      "2          30.46          1128  BROWN-FORMAN CORP            \n",
      "3          26.11          9165  ULTRA BEVERAGE COMPANY LLP   \n",
      "4          10.94          7245  PROXIMO SPIRITS INC.         \n",
      "Descargando archivo 'Carpeta de comparación/comparison_EndInvFINAL12312016_Clean.csv' desde la carpeta 'Carpeta de comparación'.\n",
      "Datos del archivo de comparación 'EndInvFINAL12312016_Clean.csv':\n",
      "         InventoryId  Store          City  Brand                  Description  \\\n",
      "0  1_HARDERSFIELD_58      1  HARDERSFIELD     58  Gekkeikan Black & Gold Sake   \n",
      "1  1_HARDERSFIELD_62      1  HARDERSFIELD     62     Herradura Silver Tequila   \n",
      "2  1_HARDERSFIELD_63      1  HARDERSFIELD     63   Herradura Reposado Tequila   \n",
      "3  1_HARDERSFIELD_72      1  HARDERSFIELD     72         No. 3 London Dry Gin   \n",
      "4  1_HARDERSFIELD_75      1  HARDERSFIELD     75    Three Olives Tomato Vodka   \n",
      "\n",
      "    Size  onHand  Price     endDate  \n",
      "0  750mL      11  12.99  2016-12-31  \n",
      "1  750mL       7  36.99  2016-12-31  \n",
      "2  750mL       7  38.99  2016-12-31  \n",
      "3  750mL       4  34.99  2016-12-31  \n",
      "4  750mL       7  14.99  2016-12-31  \n",
      "Realizando comparación entre los datos de 'InvoicePurchases12312016_Clean.csv' en la carpeta de origen y en la carpeta de comparación.\n",
      "Se encontraron filas nuevas en el archivo 'InvoicePurchases12312016_Clean.csv':\n",
      "      VendorNumber                   VendorName InvoiceDate     PONumber  \\\n",
      "0              105  ALTAMAR BRANDS LLC           2016-01-04         8124   \n",
      "1             4466  AMERICAN VINTAGE BEVERAGE    2016-01-07         8137   \n",
      "2              388  ATLANTIC IMPORTING COMPANY   2016-01-09         8169   \n",
      "3              480  BACARDI USA INC              2016-01-12         8106   \n",
      "4              516  BANFI PRODUCTS CORP          2016-01-07         8170   \n",
      "...            ...                          ...         ...          ...   \n",
      "5577          9622  WEIN BAUER INC               2017-01-06  no definido   \n",
      "5578          9625  WESTERN SPIRITS BEVERAGE CO  2017-01-10        13661   \n",
      "5579          3664  WILLIAM GRANT & SONS INC     2017-01-02        13643   \n",
      "5580          9815  WINE GROUP INC               2017-01-03        13602   \n",
      "5581         90058  ZORVINO VINEYARDS            2017-01-05        13574   \n",
      "\n",
      "           PODate      PayDate     Quantity      Dollars  Freight  \n",
      "0      2015-12-21   2016-02-16            6       214.26     3.47  \n",
      "1      2015-12-22   2016-02-21           15       140.55     8.57  \n",
      "2      2015-12-24   2016-02-16            5        106.6     4.61  \n",
      "3      2015-12-20   2016-02-05        10100    137483.78  2935.20  \n",
      "4      2015-12-24   2016-02-12         1935     15527.25   429.20  \n",
      "...           ...          ...          ...          ...      ...  \n",
      "5577   2016-12-21   2017-02-10           90       1563.0     8.60  \n",
      "5578  no definido   2017-02-18         4617     37300.48   186.50  \n",
      "5579   2016-12-22  no definido         9848    202815.78   932.95  \n",
      "5580   2016-12-20   2017-02-08  no definido    149007.56   819.54  \n",
      "5581   2016-12-18   2017-02-12          437  no definido    16.60  \n",
      "\n",
      "[5582 rows x 9 columns]\n",
      "Archivo new_InvoicePurchases12312016_Clean.csv subido a Google Drive en la carpeta especificada: 18gXDMFkbWoGDXfRN6JVcmQdi791cHZ9G\n",
      "El archivo new_InvoicePurchases12312016_Clean.csv con las filas nuevas se ha subido correctamente a la carpeta de destino.\n",
      "Realizando comparación entre los datos de 'PurchasesFINAL12312016_Clean.csv' en la carpeta de origen y en la carpeta de comparación.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 97\u001b[0m\n\u001b[0;32m     94\u001b[0m df_origen \u001b[38;5;241m=\u001b[39m origen_data[file_name]\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Comparar y obtener las filas nuevas\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m df_nuevas_filas \u001b[38;5;241m=\u001b[39m df_origen[\u001b[38;5;241m~\u001b[39m\u001b[43mdf_origen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39misin(df_comparacion\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mtuple\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))]\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m df_nuevas_filas\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSe encontraron filas nuevas en el archivo \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\frame.py:10374\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m  10360\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[0;32m  10362\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[0;32m  10363\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  10364\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10372\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m  10373\u001b[0m )\n\u001b[1;32m> 10374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[1;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\apply.py:1079\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1076\u001b[0m results \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m-> 1079\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mseries_gen\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;49;00m\n\u001b[0;32m   1081\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1082\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mABCSeries\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1083\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# If we have a view on v, we need to make a copy because\u001b[39;49;00m\n\u001b[0;32m   1084\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m#  series_generator will swap out the underlying data\u001b[39;49;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\apply.py:1271\u001b[0m, in \u001b[0;36mFrameColumnApply.series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1268\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arr, name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(values, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex):\n\u001b[0;32m   1269\u001b[0m     \u001b[38;5;66;03m# GH#35462 re-pin mgr in case setitem changed it\u001b[39;00m\n\u001b[0;32m   1270\u001b[0m     ser\u001b[38;5;241m.\u001b[39m_mgr \u001b[38;5;241m=\u001b[39m mgr\n\u001b[1;32m-> 1271\u001b[0m     \u001b[43mmgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1272\u001b[0m     \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(ser, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, name)\n\u001b[0;32m   1273\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_view:\n\u001b[0;32m   1274\u001b[0m         \u001b[38;5;66;03m# In apply_series_generator we store the a shallow copy of the\u001b[39;00m\n\u001b[0;32m   1275\u001b[0m         \u001b[38;5;66;03m# result, which potentially increases the ref count of this reused\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1278\u001b[0m         \u001b[38;5;66;03m# the refs here to avoid triggering a unnecessary CoW inside the\u001b[39;00m\n\u001b[0;32m   1279\u001b[0m         \u001b[38;5;66;03m# applied function (https://github.com/pandas-dev/pandas/pull/56212)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\internals\\managers.py:2076\u001b[0m, in \u001b[0;36mSingleBlockManager.set_values\u001b[1;34m(self, values)\u001b[0m\n\u001b[0;32m   2073\u001b[0m \u001b[38;5;66;03m# NOTE(CoW) Currently this is only used for FrameColumnApply.series_generator\u001b[39;00m\n\u001b[0;32m   2074\u001b[0m \u001b[38;5;66;03m# which handles CoW by setting the refs manually if necessary\u001b[39;00m\n\u001b[0;32m   2075\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;241m=\u001b[39m values\n\u001b[1;32m-> 2076\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_mgr_locs \u001b[38;5;241m=\u001b[39m \u001b[43mBlockPlacement\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#___________________________________________________________________________________________________________________________________________________________\n",
    "\n",
    "#                                                       LIBRERIAS NECESARIAS\n",
    "\n",
    "import os\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaFileUpload, MediaIoBaseDownload\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "\n",
    "# Directorio donde se encuentra el archivo JSON de credenciales\n",
    "directorio_credenciales = \"data-424019-28bfddebf741.json\"\n",
    "\n",
    "# Cargar credenciales desde el archivo JSON descargado\n",
    "credentials = service_account.Credentials.from_service_account_file(directorio_credenciales)\n",
    "\n",
    "# Crear cliente para acceder a la API de Google Drive\n",
    "drive_service = build('drive', 'v3', credentials=credentials)\n",
    "\n",
    "# IDs de las carpetas en Google Drive\n",
    "source_folder_id = '1tZHS0BuJoSG3DsfSmxgY5Am3Llj24JPY'  # ID de la carpeta de destino\n",
    "comparison_folder_id = '1ZMAqOISfz-z5kv_4OgGZT5lI1C9S3Xwb'  # ID de la carpeta de comparación\n",
    "destination_folder_id  = '18gXDMFkbWoGDXfRN6JVcmQdi791cHZ9G'  # ID de la carpeta de origen\n",
    "\n",
    "# Función para descargar un archivo de Google Drive\n",
    "def descargar_archivo(file_id, download_path, folder_name):\n",
    "    request = drive_service.files().get_media(fileId=file_id)\n",
    "    with open(download_path, 'wb') as fh:\n",
    "        downloader = MediaIoBaseDownload(fh, request)\n",
    "        done = False\n",
    "        while not done:\n",
    "            status, done = downloader.next_chunk()\n",
    "            print(f\"Descargando archivo '{folder_name}/{os.path.basename(download_path)}' desde la carpeta '{folder_name}'.\")\n",
    "\n",
    "# Función para subir un archivo a una carpeta específica en Google Drive\n",
    "def subir_archivo_a_carpeta(upload_file_path, folder_id, file_name):\n",
    "    media = MediaFileUpload(upload_file_path, resumable=True)\n",
    "    file_metadata = {\n",
    "        'name': file_name,\n",
    "        'parents': [folder_id]\n",
    "    }\n",
    "    drive_service.files().create(body=file_metadata, media_body=media, fields='id').execute()\n",
    "    print(f'Archivo {file_name} subido a Google Drive en la carpeta especificada: {folder_id}')\n",
    "\n",
    "# Directorio temporal para manipular archivos\n",
    "temp_dir = tempfile.gettempdir()\n",
    "\n",
    "# Obtener lista de archivos en la carpeta de origen de Google Drive\n",
    "results = drive_service.files().list(q=f\"'{source_folder_id}' in parents\", fields='files(id, name)').execute()\n",
    "source_files = results.get('files', [])\n",
    "\n",
    "# Leer los archivos de la carpeta de origen y almacenarlos en un diccionario\n",
    "origen_data = {}\n",
    "for file in source_files:\n",
    "    file_id = file['id']\n",
    "    file_name = file['name']\n",
    "    download_path_origen = os.path.join(temp_dir, file_name)\n",
    "    \n",
    "    # Descargar archivo de la carpeta de origen\n",
    "    descargar_archivo(file_id, download_path_origen, 'Carpeta de origen')\n",
    "    \n",
    "    # Leer el archivo CSV y almacenar los datos en el diccionario\n",
    "    origen_data[file_name] = pd.read_csv(download_path_origen)\n",
    "    print(f\"Datos del archivo de origen '{file_name}':\")\n",
    "    print(origen_data[file_name].head())\n",
    "\n",
    "# Obtener lista de archivos en la carpeta de comparación de Google Drive\n",
    "results_comparison = drive_service.files().list(q=f\"'{comparison_folder_id}' in parents\", fields='files(id, name)').execute()\n",
    "comparison_files_list = results_comparison.get('files', [])\n",
    "\n",
    "# Definir comparison_files como un diccionario vacío para almacenar los nombres de archivo y sus datos\n",
    "comparison_files = {}\n",
    "\n",
    "# Leer los archivos de la carpeta de comparación y almacenarlos en un diccionario\n",
    "for file in comparison_files_list:\n",
    "    file_id = file['id']\n",
    "    file_name = file['name']\n",
    "    download_path_comparacion = os.path.join(temp_dir, f\"comparison_{file_name}\")\n",
    "    \n",
    "    # Descargar archivo de la carpeta de comparación\n",
    "    descargar_archivo(file_id, download_path_comparacion, 'Carpeta de comparación')\n",
    "    \n",
    "    # Leer el archivo CSV y almacenar los datos en el diccionario\n",
    "    comparison_files[file_name] = pd.read_csv(download_path_comparacion)\n",
    "    print(f\"Datos del archivo de comparación '{file_name}':\")\n",
    "    print(comparison_files[file_name].head())\n",
    "\n",
    "# Realizar la comparación entre los datos de origen y los datos de comparación\n",
    "for file_name, df_comparacion in comparison_files.items():\n",
    "    if file_name in origen_data:\n",
    "        print(f\"Realizando comparación entre los datos de '{file_name}' en la carpeta de origen y en la carpeta de comparación.\")\n",
    "        \n",
    "        df_origen = origen_data[file_name]\n",
    "        \n",
    "        # Comparar y obtener las filas nuevas\n",
    "        df_nuevas_filas = df_origen[~df_origen.apply(tuple, axis=1).isin(df_comparacion.apply(tuple, axis=1))]\n",
    "\n",
    "        if not df_nuevas_filas.empty:\n",
    "            print(f\"Se encontraron filas nuevas en el archivo '{file_name}':\")\n",
    "            print(df_nuevas_filas)\n",
    "            # Subir el DataFrame con las filas nuevas a la carpeta de destino\n",
    "            nuevo_nombre_archivo = f\"new_{file_name}\"\n",
    "            nuevo_nombre_path = os.path.join(temp_dir, nuevo_nombre_archivo)\n",
    "            df_nuevas_filas.to_csv(nuevo_nombre_path, index=False)\n",
    "            subir_archivo_a_carpeta(nuevo_nombre_path, destination_folder_id, nuevo_nombre_archivo)\n",
    "            print(f\"El archivo {nuevo_nombre_archivo} con las filas nuevas se ha subido correctamente a la carpeta de destino.\")\n",
    "        else:\n",
    "            print(f\"No se encontraron filas nuevas en el archivo '{file_name}'.\")\n",
    "    else:\n",
    "        print(f\"No se encontró el archivo {file_name} en la carpeta de origen.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cambio de datos nulos y actualizacion del archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conexión exitosa\n",
      "Tablas en la base de datos:\n",
      "SalesFINAL12312016_Clean\n",
      "PurchasesFINAL12312016_Clean\n",
      "2017PurchasePricesDec_Clean\n",
      "BegInvFINAL12312016_Clean\n",
      "EndInvFINAL12312016_Clean\n",
      "InvoicePurchases12312016_Clean\n"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "# Función para listar las tablas en la base de datos\n",
    "def listar_tablas():\n",
    "    try:\n",
    "\n",
    "\n",
    "# Define the connection string\n",
    "        conn_str = (\n",
    "            'DRIVER={ODBC Driver 17 for SQL Server};'\n",
    "            'SERVER=JULIAN;'\n",
    "            'DATABASE=Top-Drinks;'\n",
    "            'Trusted_Connection=yes;'\n",
    "        )\n",
    "\n",
    "        # Establish a connection\n",
    "        try:\n",
    "            conn = pyodbc.connect(conn_str)\n",
    "            print(\"Conexión exitosa\")\n",
    "            # Create a cursor\n",
    "            cursor = conn.cursor()\n",
    "\n",
    "            # Ahora puedes ejecutar comandos SQL usando el cursor\n",
    "            # cursor.execute(\"SELECT * FROM tu_tabla\")\n",
    "            # rows = cursor.fetchall()\n",
    "            # for row in rows:\n",
    "            #     print(row)\n",
    "        except pyodbc.Error as e:\n",
    "            print(\"Error en la conexión a SQL Server:\", e)\n",
    "\n",
    "\n",
    "        # Now you can execute SQL commands using the cursor\n",
    "\n",
    "        # Obtener la lista de tablas\n",
    "        cursor.execute(\"SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_TYPE='BASE TABLE'\")\n",
    "        tablas = cursor.fetchall()\n",
    "\n",
    "        print(\"Tablas en la base de datos:\")\n",
    "        for tabla in tablas:\n",
    "            print(tabla[0])\n",
    "\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "    except Exception as e:\n",
    "        print(f'Error en la conexión a SQL Server: {str(e)}')\n",
    "\n",
    "# Llamar a la función para listar las tablas\n",
    "listar_tablas()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed old CSV file: 2017PurchasePricesDec_Clean.csv\n",
      "Removed old CSV file: BegInvFINAL12312016_Clean.csv\n",
      "Removed old CSV file: EndInvFINAL12312016_Clean.csv\n",
      "Removed old CSV file: InvoicePurchases12312016_Clean.csv\n",
      "Removed old CSV file: PurchasesFINAL12312016_Clean.csv\n",
      "Removed old CSV file: SalesFINAL12312016_Clean.csv\n",
      "Downloaded 28% of PurchasesFINAL12312016_Clean.csv\n",
      "Downloaded 57% of PurchasesFINAL12312016_Clean.csv\n",
      "Downloaded 86% of PurchasesFINAL12312016_Clean.csv\n",
      "Downloaded 100% of PurchasesFINAL12312016_Clean.csv\n",
      "Downloaded 82% of SalesFINAL12312016_Clean.csv\n",
      "Downloaded 100% of SalesFINAL12312016_Clean.csv\n",
      "Downloaded 100% of BegInvFINAL12312016_Clean.csv\n",
      "Downloaded 100% of EndInvFINAL12312016_Clean.csv\n",
      "Downloaded 100% of InvoicePurchases12312016_Clean.csv\n",
      "Downloaded 100% of 2017PurchasePricesDec_Clean.csv\n",
      "Se han cambiado los delimitadores de todos los archivos CSV en el directorio por ';'.\n",
      "El archivo /ruta/a/tu/directorio/base\\archivos_csv_google_drive\\old_file.csv no existe o la ruta es incorrecta.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "\n",
    "# Definir la ruta al archivo JSON de credenciales de Google Drive\n",
    "credentials_file = 'data-424019-28bfddebf741.json'\n",
    "\n",
    "# Cargar las credenciales desde el archivo JSON\n",
    "credentials = service_account.Credentials.from_service_account_file(credentials_file)\n",
    "\n",
    "# Crear un cliente para acceder a la API de Google Drive\n",
    "drive_service = build('drive', 'v3', credentials=credentials)\n",
    "\n",
    "# ID de la carpeta en Google Drive donde se encuentran los archivos CSV\n",
    "folder_id = '1tZHS0BuJoSG3DsfSmxgY5Am3Llj24JPY'\n",
    "\n",
    "# Directorio base donde se guardarán los archivos CSV descargados\n",
    "base_directory = '/ruta/a/tu/directorio/base'\n",
    "csv_folder_name = 'archivos_csv_google_drive'\n",
    "local_directory = os.path.join(base_directory, csv_folder_name)\n",
    "\n",
    "# Función para descargar archivos CSV de Google Drive\n",
    "def download_csv_files_from_drive(folder_id, local_directory):\n",
    "    # Eliminar archivos CSV antiguos en el directorio local\n",
    "    for file in os.listdir(local_directory):\n",
    "        if file.endswith('.csv'):\n",
    "            os.remove(os.path.join(local_directory, file))\n",
    "            print(f\"Removed old CSV file: {file}\")\n",
    "    \n",
    "    # Descargar los nuevos archivos CSV desde Google Drive\n",
    "    results = drive_service.files().list(q=f\"'{folder_id}' in parents\", fields='files(id, name)').execute()\n",
    "    files = results.get('files', [])\n",
    "    for file in files:\n",
    "        file_id = file['id']\n",
    "        file_name = file['name']\n",
    "        download_path = os.path.join(local_directory, file_name)\n",
    "        request = drive_service.files().get_media(fileId=file_id)\n",
    "        with open(download_path, 'wb') as fh:\n",
    "            downloader = MediaIoBaseDownload(fh, request)\n",
    "            done = False\n",
    "            while not done:\n",
    "                status, done = downloader.next_chunk()\n",
    "                print(f\"Downloaded {int(status.progress() * 100)}% of {file_name}\")\n",
    "\n",
    "    # Cambiar el delimitador de todos los archivos CSV en el directorio\n",
    "    # Obtener la lista de archivos CSV en el directorio\n",
    "    csv_files = os.listdir(local_directory)\n",
    "\n",
    "    # Iterar sobre cada archivo CSV y cambiar el delimitador\n",
    "    for file_name in csv_files:\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(local_directory, file_name)\n",
    "            # Leer el archivo CSV con la coma como delimitador original\n",
    "            df = pd.read_csv(file_path)\n",
    "            # Cambiar el delimitador de la coma a punto y coma\n",
    "            df.to_csv(file_path, sep=';', index=False)\n",
    "\n",
    "    print(\"Se han cambiado los delimitadores de todos los archivos CSV en el directorio por ';'.\")\n",
    "\n",
    "    # Verificar la existencia del archivo old_file.csv\n",
    "    old_file_path = os.path.join(local_directory, \"old_file.csv\")\n",
    "    if os.path.exists(old_file_path):\n",
    "        print(f\"El archivo {old_file_path} existe.\")\n",
    "        new_file_path = os.path.join(local_directory, \"new_file.csv\")\n",
    "        compare_and_create_new_file(old_file_path, new_file_path)\n",
    "    else:\n",
    "        print(f\"El archivo {old_file_path} no existe o la ruta es incorrecta.\")\n",
    "\n",
    "# Función para comparar archivos CSV antiguos y nuevos y crear un nuevo archivo con las filas o columnas nuevas\n",
    "def compare_and_create_new_file(old_file_path, new_file_path):\n",
    "    old_df = pd.read_csv(old_file_path)\n",
    "    new_df = pd.read_csv(new_file_path)\n",
    "    \n",
    "    # Comparar los DataFrames y encontrar las filas nuevas en el archivo nuevo\n",
    "    new_rows = new_df[~new_df.isin(old_df)].dropna()\n",
    "    \n",
    "    # Guardar las filas nuevas en un nuevo archivo CSV\n",
    "    new_file_name = os.path.basename(old_file_path)  # Utilizar el nombre del archivo viejo\n",
    "    new_file_path = os.path.join(local_directory, new_file_name)\n",
    "    new_rows.to_csv(new_file_path, index=False)\n",
    "    print(f\"Se han creado el archivo {new_file_path} con las filas nuevas.\")\n",
    "\n",
    "# Ejemplo de uso\n",
    "if __name__ == \"__main__\":\n",
    "    # Descargar archivos CSV de Google Drive a un directorio local\n",
    "    download_csv_files_from_drive(folder_id, local_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 94) (170457041.py, line 94)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[28], line 94\u001b[1;36m\u001b[0m\n",
      "\u001b[1;33m    params = urllib.parse.quote_plus(\"DRIVER={ODBC Driver 17 for SQL Server};SERVER=JULIAN;DATABASE=Top-Drinks;Trusted_Connection=yes )\u001b[0m\n",
      "\u001b[1;37m                                     ^\u001b[0m\n",
      "\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated string literal (detected at line 94)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "\n",
    "# Definir la ruta al archivo JSON de credenciales de Google Drive\n",
    "credentials_file = 'data-424019-28bfddebf741.json'\n",
    "\n",
    "# Cargar las credenciales desde el archivo JSON\n",
    "credentials = service_account.Credentials.from_service_account_file(credentials_file)\n",
    "\n",
    "# Crear un cliente para acceder a la API de Google Drive\n",
    "drive_service = build('drive', 'v3', credentials=credentials)\n",
    "\n",
    "# ID de la carpeta en Google Drive donde se encuentran los archivos CSV\n",
    "folder_id = '1tZHS0BuJoSG3DsfSmxgY5Am3Llj24JPY'\n",
    "\n",
    "# Directorio base donde se guardarán los archivos CSV descargados\n",
    "base_directory = '/ruta/a/tu/directorio/base'\n",
    "csv_folder_name = 'archivos_csv_google_drive'\n",
    "local_directory = os.path.join(base_directory, csv_folder_name)\n",
    "\n",
    "# Función para descargar archivos CSV de Google Drive\n",
    "def download_csv_files_from_drive(folder_id, local_directory):\n",
    "    print(\"Descargando archivos CSV desde Google Drive...\")\n",
    "    \n",
    "    # Eliminar archivos CSV antiguos en el directorio local\n",
    "    for file in os.listdir(local_directory):\n",
    "        if file.endswith('.csv'):\n",
    "            os.remove(os.path.join(local_directory, file))\n",
    "            print(f\"Removed old CSV file: {file}\")\n",
    "    \n",
    "    # Descargar los nuevos archivos CSV desde Google Drive\n",
    "    results = drive_service.files().list(q=f\"'{folder_id}' in parents\", fields='files(id, name)').execute()\n",
    "    files = results.get('files', [])\n",
    "    for file in files:\n",
    "        file_id = file['id']\n",
    "        file_name = file['name']\n",
    "        download_path = os.path.join(local_directory, file_name)\n",
    "        request = drive_service.files().get_media(fileId=file_id)\n",
    "        with open(download_path, 'wb') as fh:\n",
    "            downloader = MediaIoBaseDownload(fh, request)\n",
    "            done = False\n",
    "            while not done:\n",
    "                status, done = downloader.next_chunk()\n",
    "                print(f\"Downloaded {int(status.progress() * 100)}% of {file_name}\")\n",
    "\n",
    "    # Cambiar el delimitador de todos los archivos CSV en el directorio\n",
    "    print(\"Cambiando delimitadores de archivos CSV...\")\n",
    "    for file_name in os.listdir(local_directory):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(local_directory, file_name)\n",
    "            # Leer el archivo CSV con la coma como delimitador original\n",
    "            df = pd.read_csv(file_path)\n",
    "            # Cambiar el delimitador de la coma a punto y coma\n",
    "            df.to_csv(file_path, sep=';', index=False)\n",
    "\n",
    "    print(\"Se han cambiado los delimitadores de todos los archivos CSV en el directorio por ';'.\")\n",
    "\n",
    "# Función para cargar datos desde archivos CSV a una base de datos SQL Server\n",
    "def load_csv_data_to_sql(csv_directory, conn_str):\n",
    "    print(\"Cargando datos desde archivos CSV a SQL Server...\")\n",
    "    for file in os.listdir(csv_directory):\n",
    "        if file.endswith('.csv'):\n",
    "            file_path = os.path.join(csv_directory, file)\n",
    "            table_name = os.path.splitext(file)[0]  # Utilizar el nombre del archivo CSV como nombre de la tabla\n",
    "            \n",
    "            # Define el delimitador predeterminado\n",
    "            delimiter = ';'\n",
    "\n",
    "            # Define una función de redondeo personalizada\n",
    "            def custom_round(value):\n",
    "                if isinstance(value, float):\n",
    "                    return round(value, 2)\n",
    "                return value\n",
    "            \n",
    "            # Leer el archivo CSV con el delimitador correcto y aplicar la función de redondeo personalizada\n",
    "            df = pd.read_csv(file_path, delimiter=delimiter, converters={i: custom_round for i in range(10)})  # ajusta el rango 10\n",
    "            \n",
    "            # Crear el motor de conexión a la base de datos\n",
    "            engine = create_engine(conn_str)\n",
    "            \n",
    "            # Cargar los datos en SQL Server\n",
    "            df.to_sql(name=table_name, con=engine, if_exists='append', index=False)\n",
    "            print(f\"Data from {file} loaded into SQL table {table_name}\")\n",
    "\n",
    "# Ejemplo de uso\n",
    "if __name__ == \"__main__\":\n",
    "    # Descargar archivos CSV de Google Drive a un directorio local\n",
    "    download_csv_files_from_drive(folder_id, local_directory)\n",
    "    \n",
    "    # Construir la cadena de conexión para SQLAlchemy\n",
    "    params = urllib.parse.quote_plus(\"DRIVER={ODBC Driver 17 for SQL Server};SERVER=JULIAN;DATABASE=Top-Drinks;Trusted_Connection=yes )\n",
    "    conn_str = f\"mssql+pyodbc:///?odbc_connect={params}\"\n",
    "    \n",
    "    # Cargar datos desde archivos CSV a una base de datos SQL Server\n",
    "    load_csv_data_to_sql(local_directory, conn_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### no funciono\n",
    "##### cambiar el tipo de datos desde python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed old CSV file: 2017PurchasePricesDec_Clean.csv\n",
      "Removed old CSV file: BegInvFINAL12312016_Clean.csv\n",
      "Removed old CSV file: EndInvFINAL12312016_Clean.csv\n",
      "Removed old CSV file: InvoicePurchases12312016_Clean.csv\n",
      "Removed old CSV file: PurchasesFINAL12312016_Clean.csv\n",
      "Removed old CSV file: SalesFINAL12312016_Clean.csv\n",
      "Tabla: SalesFINAL12312016_Clean\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/julia/Documents\\\\archivos_csv_google_drive\\\\SalesFINAL12312016_Clean.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 237\u001b[0m\n\u001b[0;32m    234\u001b[0m local_directory \u001b[38;5;241m=\u001b[39m establecer_ruta_base(base_directory, csv_folder_name)\n\u001b[0;32m    236\u001b[0m \u001b[38;5;66;03m# Descargar archivos CSV de Google Drive a un directorio local\u001b[39;00m\n\u001b[1;32m--> 237\u001b[0m \u001b[43mdownload_csv_files_from_drive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_directory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;66;03m# Construir la cadena de conexión para SQLAlchemy\u001b[39;00m\n\u001b[0;32m    240\u001b[0m params \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mparse\u001b[38;5;241m.\u001b[39mquote_plus(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDRIVER=\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124mODBC Driver 17 for SQL Server};SERVER=JULIAN;DATABASE=Top-Drinks;Trusted_Connection=yes;\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 78\u001b[0m, in \u001b[0;36mdownload_csv_files_from_drive\u001b[1;34m(folder_id, local_directory)\u001b[0m\n\u001b[0;32m     75\u001b[0m last_download_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# Llamar a la función para imprimir los tipos de datos de cada columna en cada tabla\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m \u001b[43mprint_column_types\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_directory\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 123\u001b[0m, in \u001b[0;36mprint_column_types\u001b[1;34m(local_directory)\u001b[0m\n\u001b[0;32m    120\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(local_directory, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# Leer el archivo CSV\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# Aplicar los tipos de datos especificados en column_types al DataFrame\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col, dtype \u001b[38;5;129;01min\u001b[39;00m columns\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/julia/Documents\\\\archivos_csv_google_drive\\\\SalesFINAL12312016_Clean.csv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "import pyodbc\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Definir la ruta al archivo JSON de credenciales de Google Drive\n",
    "credentials_file = 'data-424019-28bfddebf741.json'\n",
    "\n",
    "# Cargar las credenciales desde el archivo JSON\n",
    "credentials = service_account.Credentials.from_service_account_file(credentials_file)\n",
    "\n",
    "# Crear un cliente para acceder a la API de Google Drive\n",
    "drive_service = build('drive', 'v3', credentials=credentials)\n",
    "\n",
    "# ID de la carpeta en Google Drive donde se encuentran los archivos CSV\n",
    "folder_id = '1tZHS0BuJoSG3DsfSmxgY5Am3Llj24JPY'\n",
    "\n",
    "# Directorio base donde se guardarán los archivos CSV descargados\n",
    "base_directory = 'C:/Users/julia/Documents'  # Cambiar por la ruta correcta\n",
    "csv_folder_name = 'archivos_csv_google_drive'\n",
    "\n",
    "# Función para establecer y validar la ruta base\n",
    "def establecer_ruta_base(base_path, folder_name):\n",
    "    local_directory = os.path.join(base_path, folder_name)\n",
    "    if not os.path.exists(local_directory):\n",
    "        os.makedirs(local_directory)\n",
    "    return local_directory\n",
    "\n",
    "# Variable global para almacenar la hora de la última descarga\n",
    "last_download_time = None\n",
    "\n",
    "# Función para descargar archivos CSV de Google Drive\n",
    "def download_csv_files_from_drive(folder_id, local_directory):\n",
    "    global last_download_time\n",
    "    \n",
    "    # Eliminar archivos CSV antiguos en el directorio local\n",
    "    for file in os.listdir(local_directory):\n",
    "        if file.endswith('.csv'):\n",
    "            os.remove(os.path.join(local_directory, file))\n",
    "            print(f\"Removed old CSV file: {file}\")\n",
    "    \n",
    "    # Verificar si se ha descargado en la última hora\n",
    "    if last_download_time is not None and datetime.now() - last_download_time < timedelta(hours=1):\n",
    "        print(\"CSV files were downloaded less than an hour ago. Skipping download.\")\n",
    "        return\n",
    "    \n",
    "    # Obtener la hora actual menos 1 hora\n",
    "    one_hour_ago = datetime.now() - timedelta(hours=1)\n",
    "    \n",
    "    # Descargar los nuevos archivos CSV desde Google Drive\n",
    "    results = drive_service.files().list(q=f\"'{folder_id}' in parents\", fields='files(id, name, modifiedTime)').execute()\n",
    "    files = results.get('files', [])\n",
    "    for file in files:\n",
    "        file_id = file['id']\n",
    "        file_name = file['name']\n",
    "        modified_time = datetime.fromisoformat(file['modifiedTime'][:-1])  # Eliminar la 'Z' del final del timestamp\n",
    "        # Descargar solo si el archivo ha sido modificado en la última hora\n",
    "        if modified_time > one_hour_ago:\n",
    "            download_path = os.path.join(local_directory, file_name)\n",
    "            request = drive_service.files().get_media(fileId=file_id)\n",
    "            with open(download_path, 'wb') as fh:\n",
    "                downloader = MediaIoBaseDownload(fh, request)\n",
    "                done = False\n",
    "                while not done:\n",
    "                    status, done = downloader.next_chunk()\n",
    "                    print(f\"Downloaded {int(status.progress() * 100)}% of {file_name}\")\n",
    "\n",
    "        \n",
    "    # Actualizar la hora de la última descarga\n",
    "    last_download_time = datetime.now()\n",
    "\n",
    "    # Llamar a la función para imprimir los tipos de datos de cada columna en cada tabla\n",
    "    print_column_types(local_directory)\n",
    "\n",
    "\n",
    "\n",
    "# Función para cargar datos desde archivos CSV a una base de datos SQL Server\n",
    "def load_csv_data_to_sql(csv_directory, conn_str):\n",
    "    files = os.listdir(csv_directory)\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            file_path = os.path.join(csv_directory, file)\n",
    "            table_name = os.path.splitext(file)[0]  # Utilizar el nombre del archivo CSV como nombre de la tabla\n",
    "            \n",
    "            # Verificar si existe un mapeo de tipos de datos para esta tabla\n",
    "            if table_name in column_types:\n",
    "                # Obtener el mapeo de tipos de datos para esta tabla\n",
    "                columns = column_types[table_name]\n",
    "                \n",
    "                # Leer el archivo CSV con el delimitador correcto\n",
    "                delimiter = ';'  # Delimitador predeterminado\n",
    "                if table_name in ['BegInvFINAL12312016_Clean', 'EndInvFINAL12312016_Clean']:\n",
    "                    delimiter = ','  # Cambiar el delimitador para archivos específicos\n",
    "                df = pd.read_csv(file_path, delimiter=delimiter)\n",
    "                \n",
    "                # Aplicar conversiones de tipo de datos según el mapeo\n",
    "                for column, dtype in columns.items():\n",
    "                    if column in df.columns:\n",
    "                        df[column] = df[column].astype(dtype)\n",
    "                \n",
    "                # Crear el motor de conexión a la base de datos\n",
    "                engine = create_engine(conn_str)\n",
    "                \n",
    "                # Cargar los datos en SQL Server\n",
    "                df.to_sql(name=table_name, con=engine, if_exists='replace', index=False)\n",
    "                print(f\"Data from {file} loaded into SQL table {table_name}\")\n",
    "            else:\n",
    "                print(f\"No column type mapping found for table {table_name}\")\n",
    "\n",
    "\n",
    "                \n",
    "def print_column_types(local_directory):\n",
    "    for table_name, columns in column_types.items():\n",
    "        print(f\"Tabla: {table_name}\")\n",
    "        file_path = os.path.join(local_directory, f\"{table_name}.csv\")\n",
    "        \n",
    "        # Leer el archivo CSV\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Aplicar los tipos de datos especificados en column_types al DataFrame\n",
    "        for col, dtype in columns.items():\n",
    "            if col in df.columns:\n",
    "                if dtype == 'datetime':\n",
    "                    try:\n",
    "                        df[col] = pd.to_datetime(df[col])\n",
    "                    except ValueError:\n",
    "                        print(f\"Error al convertir la columna {col} a tipo de dato datetime en la tabla {table_name}\")\n",
    "                elif dtype == 'nvarchar(max)':\n",
    "                    df[col] = df[col].astype('object')  # Convertir a tipo 'object'\n",
    "                else:\n",
    "                    # Convertir valores no numéricos o desconocidos a NaN antes de la conversión\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "                    # Luego, convertir a tipo de dato especificado\n",
    "                    df[col] = df[col].astype(dtype)\n",
    "        \n",
    "        # Imprimir los tipos de datos actualizados\n",
    "        print(df.dtypes)\n",
    "        print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Crear un diccionario con los nombres de las columnas y los tipos de datos asociados\n",
    "column_types = {\n",
    "    'SalesFINAL12312016_Clean': {\n",
    "        'InventoryId': 'nvarchar(max)',\n",
    "        'Store': 'int',\n",
    "        'Brand': 'int',\n",
    "        'Description': 'nvarchar(max)',\n",
    "        'Size': 'nvarchar(max)',\n",
    "        'SalesQuantity': 'int',\n",
    "        'SalesDollars': 'float',\n",
    "        'SalesPrice': 'float',\n",
    "        'SalesDate': 'datetime',\n",
    "        'Volume': 'int',\n",
    "        'Classification': 'int',\n",
    "        'ExciseTax': 'float',\n",
    "        'VendorNo': 'int',\n",
    "        'VendorName': 'nvarchar(max)'\n",
    "    },\n",
    "    'BegInvFINAL12312016_Clean': {\n",
    "        'InventoryId': 'nvarchar(max)',\n",
    "        'Store': 'int',\n",
    "        'City': 'nvarchar(max)',\n",
    "        'Brand': 'int',\n",
    "        'Description': 'nvarchar(max)',\n",
    "        'Size': 'nvarchar(max)',\n",
    "        'onHand': 'int',\n",
    "        'Price': 'float',\n",
    "        'startDate': 'datetime'\n",
    "        },\n",
    "    'InvoicePurchases12312016_Clean': {\n",
    "        'VendorNumber': 'int',\n",
    "        'VendorName': 'nvarchar(max)',\n",
    "        'InvoiceDate': 'datetime',\n",
    "        'PONumber': 'int',\n",
    "        'PODate': 'datetime',\n",
    "        'PayDate': 'datetime',\n",
    "        'Quantity': 'int',\n",
    "        'Dollars': 'float',\n",
    "        'Freight': 'float'\n",
    "    },\n",
    "    '2017PurchasePricesDec_Clean': {\n",
    "        'Brand': 'int',\n",
    "        'Description': 'nvarchar(max)',\n",
    "        'Price': 'float',\n",
    "        'Size': 'nvarchar(max)',\n",
    "        'Volume': 'int',\n",
    "        'Classification': 'int',\n",
    "        'PurchasePrice': 'float',\n",
    "        'VendorNumber': 'int',\n",
    "        'VendorName': 'nvarchar(max)'\n",
    "    },\n",
    "    'PurchasesFINAL12312016_Clean': {\n",
    "        'InventoryId': 'nvarchar(max)',\n",
    "        'Store': 'int',\n",
    "        'Brand': 'int',\n",
    "        'Description': 'nvarchar(max)',\n",
    "        'Size': 'nvarchar(max)',\n",
    "        'VendorNumber': 'int',\n",
    "        'VendorName': 'nvarchar(max)',\n",
    "        'PONumber': 'int',\n",
    "        'PODate': 'datetime',\n",
    "        'ReceivingDate': 'datetime',\n",
    "        'InvoiceDate': 'datetime',\n",
    "        'PayDate': 'datetime',\n",
    "        'PurchasePrice': 'float',\n",
    "        'Quantity': 'int',\n",
    "        'Dollars': 'float',\n",
    "        'Classification': 'int'\n",
    "    },\n",
    "    'EndInvFINAL12312016_Clean': {\n",
    "        'InventoryId': 'nvarchar(max)',\n",
    "        'Store': 'int',\n",
    "        'City': 'nvarchar(max)',\n",
    "        'Brand': 'int',\n",
    "        'Description': 'nvarchar(max)',\n",
    "        'Size': 'nvarchar(max)',\n",
    "        'onHand': 'int',\n",
    "        'Price': 'float',\n",
    "        'endDate': 'datetime'\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Establecer y validar la ruta base\n",
    "    local_directory = establecer_ruta_base(base_directory, csv_folder_name)\n",
    "    \n",
    "    # Descargar archivos CSV de Google Drive a un directorio local\n",
    "    download_csv_files_from_drive(folder_id, local_directory)\n",
    "    \n",
    "    # Construir la cadena de conexión para SQLAlchemy\n",
    "    params = urllib.parse.quote_plus(\"DRIVER={ODBC Driver 17 for SQL Server};SERVER=JULIAN;DATABASE=Top-Drinks;Trusted_Connection=yes;\")\n",
    "    conn_str = f\"mssql+pyodbc:///?odbc_connect={params}\"\n",
    "    \n",
    "    # Cargar datos desde archivos CSV a una base de datos SQL Server\n",
    "    load_csv_data_to_sql(local_directory, conn_str)\n",
    "    \n",
    "    # Llamar a la función para listar las tablas en la base de datos SQL Server\n",
    "    listar_tablas()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'file_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Leer el archivo CSV con las columnas de fecha analizadas correctamente\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[43mfile_path\u001b[49m, dayfirst\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnas presentes en el archivo CSV:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'file_path' is not defined"
     ]
    }
   ],
   "source": [
    "# Leer el archivo CSV con las columnas de fecha analizadas correctamente\n",
    "df = pd.read_csv(file_path, dayfirst=True)\n",
    "print(\"Columnas presentes en el archivo CSV:\")\n",
    "print(df.columns)\n",
    "df = df.astype(columns)\n",
    "print(df.dtypes)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Directorio donde se encuentra el archivo JSON de credenciales\n",
    "directorio_credenciales = \"data-424019-28bfddebf741.json\"\n",
    "\n",
    "# Cargar credenciales desde el archivo JSON descargado\n",
    "credentials = service_account.Credentials.from_service_account_file(directorio_credenciales)\n",
    "\n",
    "# Crear cliente para acceder a la API de Google Drive\n",
    "drive_service = build('drive', 'v3', credentials=credentials)\n",
    "\n",
    "# ID del archivo a descargar\n",
    "file_id = 'your-file-id'\n",
    "download_path = 'downloaded_file.xlsx'  # Puedes cambiar la extensión según el tipo de archivo\n",
    "\n",
    "# Función para descargar un archivo de Google Drive\n",
    "def descargar_archivo(file_id, download_path):\n",
    "    request = drive_service.files().get_media(fileId=file_id)\n",
    "    with open(download_path, 'wb') as fh:\n",
    "        downloader = MediaIoBaseDownload(fh, request)\n",
    "        done = False\n",
    "        while not done:\n",
    "            status, done = downloader.next_chunk()\n",
    "            print(f\"Descargado {int(status.progress() * 100)}%.\")\n",
    "\n",
    "# Descargar el archivo\n",
    "descargar_archivo(file_id, download_path)\n",
    "\n",
    "# Determinar el tipo de archivo y leerlo con pandas\n",
    "if download_path.endswith('.csv'):\n",
    "    df = pd.read_csv(download_path)\n",
    "elif download_path.endswith('.xlsx'):\n",
    "    df = pd.read_excel(download_path)\n",
    "else:\n",
    "    raise ValueError(\"Formato de archivo no soportado\")\n",
    "\n",
    "# Mostrar tipos de datos de cada columna\n",
    "print(df.dtypes)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
